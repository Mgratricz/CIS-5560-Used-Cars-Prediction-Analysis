{
  "metadata": {
    "name": "ACTUALFINALRANDOMFORESTREGRESSION",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nIS_SPARK_SUBMIT_CLI \u003d False\n\nfrom pyspark import SparkContext\nsc \u003d SparkContext.getOrCreate()\nspark \u003d SparkSession(sc)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 1. Load and preview data\nfile_path \u003d \"/user/apang5/used_cars_sample_data--01percent.csv\"\ndf \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\ndf.printSchema()\ndf.show(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 2. Feature selection \u0026 cleaning\nfrom pyspark.sql.functions import col\n\n\ndata \u003d df.select(\n    \"city_fuel_economy\",\"highway_fuel_economy\",\"daysonmarket\",\n    \"engine_displacement\",\"horsepower\",\"mileage\",\"seller_rating\",\n    \"year\",\"price\",\"make_name\",\"model_name\",\n    \"torque\",\"engine_cylinders\",\"power\",\"wheelbase\",\"width\"\n)\n\n\nfor name, dtype in [\n    (\"city_fuel_economy\", \"double\"),\n    (\"highway_fuel_economy\",\"double\"),\n    (\"daysonmarket\",\"int\"),\n    (\"engine_displacement\",\"double\"),\n    (\"horsepower\",\"double\"),\n    (\"mileage\",\"double\"),\n    (\"seller_rating\",\"double\"),\n    (\"year\",\"int\"),\n    (\"price\",\"double\"),\n    (\"torque\",\"double\"),\n    (\"engine_cylinders\",\"double\"),\n    (\"power\",\"double\"),\n    (\"wheelbase\",\"double\"),\n    (\"width\",\"double\"),\n]:\n    data \u003d data.withColumn(name, col(name).cast(dtype))\n\n\nessential \u003d [\n    \"city_fuel_economy\",\"highway_fuel_economy\",\"daysonmarket\",\n    \"engine_displacement\",\"horsepower\",\"mileage\",\"seller_rating\",\n    \"year\",\"price\"\n]\ndata \u003d data.dropna(subset\u003dessential)\n\n\nnew_cols \u003d [\"torque\",\"engine_cylinders\",\"power\",\"wheelbase\",\"width\"]\nimpute_map \u003d {}\nfor c in new_cols:\n   \n    medians \u003d data.stat.approxQuantile(c, [0.5], 0.001)\n    impute_map[c] \u003d medians[0] if medians else 0.0\n\ndata \u003d data.na.fill(impute_map)\n\nprint(f\"✅ Rows remaining after cleaning: {data.count()}\")\ndata.printSchema()\n\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 3. Index categorical features\nfrom pyspark.ml.feature import StringIndexer\n\nmake_indexer  \u003d StringIndexer(inputCol\u003d\"make_name\",  outputCol\u003d\"make_indexed\")\nmodel_indexer \u003d StringIndexer(inputCol\u003d\"model_name\", outputCol\u003d\"model_indexed\")\n\ndata \u003d make_indexer.fit(data).transform(data)\ndata \u003d model_indexer.fit(data).transform(data)\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 4. Assemble + scale\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\n\nfeature_columns \u003d [\n    \"city_fuel_economy\",\"highway_fuel_economy\",\"daysonmarket\",\n    \"engine_displacement\",\"horsepower\",\"mileage\",\"seller_rating\",\n    \"year\",\"make_indexed\",\"model_indexed\",\n    \"torque\",\"engine_cylinders\",\"power\",\"wheelbase\",\"width\"\n]\n\nassembler \u003d VectorAssembler(\n    inputCols\u003dfeature_columns,\n    outputCol\u003d\"assembled_features\"\n)\nassembled_data \u003d assembler.transform(data)\n\nscaler \u003d StandardScaler(\n    inputCol\u003d\"assembled_features\",\n    outputCol\u003d\"features\",\n    withMean\u003dTrue,\n    withStd\u003dTrue\n)\nscaler_model \u003d scaler.fit(assembled_data)\nfinal_data   \u003d scaler_model.transform(assembled_data)\n\n\nfinal_data.select(\"features\").show(3, truncate\u003dFalse)\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 5. Train/test split\ntrain_data, test_data \u003d final_data.randomSplit([0.8, 0.2], seed\u003d42)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nimport time\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# 6A. Cross-Validation\nrf \u003d RandomForestRegressor(featuresCol\u003d\"features\", labelCol\u003d\"price\")\nparamGrid \u003d ParamGridBuilder() \\\n    .addGrid(rf.numTrees, [50]) \\\n    .addGrid(rf.maxDepth, [5, 10]) \\\n    .build()\n\nevaluator \u003d RegressionEvaluator(labelCol\u003d\"price\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\ncrossval \u003d CrossValidator(estimator\u003drf,\n                          estimatorParamMaps\u003dparamGrid,\n                          evaluator\u003devaluator,\n                          numFolds\u003d3)\n\ncv_start \u003d time.time()\ncv_model \u003d crossval.fit(train_data)\ncv_end \u003d time.time()\ncv_time \u003d cv_end - cv_start\n\ncv_preds \u003d cv_model.transform(test_data)\ncv_rmse \u003d evaluator.evaluate(cv_preds)\ncv_r2   \u003d evaluator.setMetricName(\"r2\").evaluate(cv_preds)\n\nprint(f\"Cross-Validated RMSE: {cv_rmse:.2f}\")\nprint(f\"Cross-Validated R2:   {cv_r2:.4f}\")\nprint(f\"CV Time: {cv_time:.2f} seconds\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# 6B. TrainValidationSplit\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport time\n\n\ne_rmse \u003d RegressionEvaluator(\n    labelCol\u003d\"price\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"rmse\"\n)\ne_r2 \u003d RegressionEvaluator(\n    labelCol\u003d\"price\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"r2\"\n)\n\ntvs \u003d TrainValidationSplit(\n    estimator\u003drf,\n    estimatorParamMaps\u003dparamGrid,\n    evaluator\u003de_rmse,      \n    trainRatio\u003d0.8\n)\n\ntvs_start \u003d time.time()\ntvs_model \u003d tvs.fit(train_data)\ntvs_end \u003d time.time()\ntvs_time \u003d tvs_end - tvs_start\n\ntvs_preds \u003d tvs_model.transform(test_data)\n\n\ntvs_rmse \u003d e_rmse.evaluate(tvs_preds)\ntvs_r2   \u003d e_r2.evaluate(tvs_preds)\n\nprint(f\"TrainValidationSplit RMSE: {tvs_rmse:.2f}\")\nprint(f\"TrainValidationSplit R2:   {tvs_r2:.4f}\")\nprint(f\"TVS Time: {tvs_time:.2f} seconds\")\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 7. Final Random Forest model training\nrf_final \u003d RandomForestRegressor(featuresCol\u003d\"features\", labelCol\u003d\"price\", numTrees\u003d100, maxDepth\u003d10)\nrf_model \u003d rf_final.fit(train_data)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 8. Final model evaluation\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\nrmse_evaluator \u003d RegressionEvaluator(\n    labelCol\u003d\"price\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"rmse\"\n)\nr2_evaluator \u003d RegressionEvaluator(\n    labelCol\u003d\"price\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"r2\"\n)\n\n\npredictions \u003d rf_model.transform(test_data)\n\n\nfinal_rmse \u003d rmse_evaluator.evaluate(predictions)\nfinal_r2   \u003d r2_evaluator.evaluate(predictions)\n\nprint(f\"Final RMSE: {final_rmse:.2f}\")\nprint(f\"Final R2:   {final_r2:.4f}\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 9. Feature importance\nimportances \u003d rf_model.featureImportances.toArray()\nfor feature, score in zip(feature_columns, importances):\n    print(f\"{feature}: {score:.4f}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# --- Summary of All Results ---\nprint(f\"Cross-Validated RMSE: {cv_rmse:.2f}\")\nprint(f\"Cross-Validated R2:   {cv_r2:.4f}\")\nprint(f\"CV Time: {cv_time:.2f} seconds\")\nprint(f\"TrainValidationSplit RMSE: {tvs_rmse:.2f}\")\nprint(f\"TrainValidationSplit R2:   {tvs_r2:.4f}\")\nprint(f\"TVS Time: {tvs_time:.2f} seconds\")\nprint(f\"Final RMSE: {final_rmse:.2f}\")\nprint(f\"Final R2:   {final_r2:.4f}\")\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}