{
  "metadata": {
    "name": "RFClassifierProjectAlgo",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Overview\n\nThis notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\nfrom pyspark.sql.functions import regexp_extract, col, when\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\nimport time\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# True when to create Python soure code to run with spark-submit \nIS_SPARK_SUBMIT_CLI \u003d False\n\nif IS_SPARK_SUBMIT_CLI:\n    sc \u003d SparkContext.getOrCreate()\n    spark \u003d SparkSession(sc)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# File location and type\nfile_location \u003d \"/user/apang5/used_cars_sample_data--01percent.csv\"\nfile_type \u003d \"csv\"\n\n# CSV options\ninfer_schema \u003d \"true\"\nfirst_row_is_header \u003d \"true\"\ndelimiter \u003d \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf \u003d spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Load the CSV with safe parsing options\ndf \u003d spark.read.format(file_type) \\\n .option(\"header\", \"true\") \\\n .option(\"inferSchema\", \"true\") \\\n .option(\"sep\", \",\") \\\n .option(\"quote\", \"\\\"\") \\\n .option(\"escape\", \"\\\"\") \\\n .option(\"multiLine\", \"true\") \\\n .option(\"mode\", \"PERMISSIVE\") \\\n .load(file_location)\n\ndf.printSchema()\ndf.show(5, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# -----------------------------------------\n# 1. Data Preparation (cleaning, as before)\n# -----------------------------------------\ndata \u003d df.select(\"city_fuel_economy\", \"highway_fuel_economy\", \"daysonmarket\",\n                 \"engine_displacement\", \"horsepower\", \"mileage\", \"seller_rating\",\n                 \"year\", \"price\", \"engine_cylinders\", \"torque\", \"power\",\n                 \"front_legroom\", \"wheelbase\", \"width\", \"body_type\", \"has_accidents\", \n                 \u0027fuel_type\u0027, \u0027transmission\u0027, \u0027make_name\u0027, \u0027model_name\u0027, \u0027exterior_color\u0027,\n                 \u0027interior_color\u0027, \u0027dealer_zip\u0027, \u0027franchise_make\u0027, \u0027wheel_system\u0027)\n\n# Cast numeric columns properly\ndata \u003d data.withColumn(\"city_fuel_economy\", col(\"city_fuel_economy\").cast(\"double\")) \\\n           .withColumn(\"highway_fuel_economy\", col(\"highway_fuel_economy\").cast(\"double\")) \\\n           .withColumn(\"daysonmarket\", col(\"daysonmarket\").cast(\"int\")) \\\n           .withColumn(\"engine_displacement\", col(\"engine_displacement\").cast(\"double\")) \\\n           .withColumn(\"horsepower\", col(\"horsepower\").cast(\"double\")) \\\n           .withColumn(\"mileage\", col(\"mileage\").cast(\"double\")) \\\n           .withColumn(\"seller_rating\", col(\"seller_rating\").cast(\"double\")) \\\n           .withColumn(\"year\", col(\"year\").cast(\"int\")) \\\n           .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n           .withColumn(\"engine_cylinders\", regexp_extract(\"engine_cylinders\", \"(\\\\d+)\", 1).cast(\"double\")) \\\n           .withColumn(\"torque\", regexp_extract(\"torque\", \"(\\\\d+)\", 1).cast(\"double\")) \\\n           .withColumn(\"power\", regexp_extract(\"power\", \"(\\\\d+)\", 1).cast(\"double\"))\n\nfor col_name in [\"front_legroom\", \"wheelbase\", \"width\"]:\n    data \u003d data.withColumn(col_name, regexp_extract(col_name, \"(\\\\d+\\\\.\\\\d+)\", 1).cast(\"double\"))\n\ndata \u003d data.withColumn(\"has_accidents\", col(\"has_accidents\").cast(\"int\"))\ndata \u003d data.dropna()\ndata.groupBy(\"has_accidents\").count().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# -----------------------------------------\n# Feature Preparation\n# -----------------------------------------\nfeature_cols \u003d [\n    \u0027city_fuel_economy\u0027, \u0027highway_fuel_economy\u0027, \u0027engine_displacement\u0027,\n    \u0027horsepower\u0027, \u0027seller_rating\u0027, \u0027year\u0027, \u0027mileage\u0027,\n    \u0027engine_cylinders\u0027, \u0027torque\u0027, \u0027power\u0027, \u0027front_legroom\u0027, \u0027wheelbase\u0027, \u0027width\u0027\n]\n\nnumeric_features \u003d [\n    \u0027city_fuel_economy\u0027, \u0027highway_fuel_economy\u0027, \u0027engine_displacement\u0027,\n    \u0027horsepower\u0027, \u0027seller_rating\u0027, \u0027year\u0027, \u0027mileage\u0027,\n    \u0027engine_cylinders\u0027, \u0027torque\u0027, \u0027power\u0027, \u0027front_legroom\u0027, \u0027wheelbase\u0027, \u0027width\u0027\n]\n\ncategorical_features \u003d [\n    \u0027body_type\u0027, \u0027fuel_type\u0027, \u0027transmission\u0027, \u0027make_name\u0027, \u0027model_name\u0027, \u0027exterior_color\u0027,\n    \u0027interior_color\u0027, \u0027dealer_zip\u0027, \u0027franchise_make\u0027, \u0027wheel_system\u0027\n]"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# \u003d\u003d\u003d Index + One-Hot Encode categorical features \u003d\u003d\u003d\nindexers \u003d [StringIndexer(inputCol\u003dcol, outputCol\u003df\"{col}_idx\", handleInvalid\u003d\"keep\") for col in categorical_features]\nencoders \u003d [OneHotEncoder(inputCol\u003df\"{col}_idx\", outputCol\u003df\"{col}_vec\") for col in categorical_features]\nencoded_features \u003d [f\"{col}_vec\" for col in categorical_features]\n\n# \u003d\u003d\u003d Assemble and scale numeric features \u003d\u003d\u003d\nnum_assembler \u003d VectorAssembler(inputCols\u003dnumeric_features, outputCol\u003d\"num_features\")\nscaler \u003d MinMaxScaler(inputCol\u003d\"num_features\", outputCol\u003d\"scaled_features\")\n\n# \u003d\u003d\u003d Final feature assembler \u003d\u003d\u003d\nfinal_assembler \u003d VectorAssembler(inputCols\u003dencoded_features + [\"scaled_features\"], outputCol\u003d\"features\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsplits \u003d data.randomSplit([0.7, 0.3])\ntrain \u003d splits[0]\ntest \u003d splits[1]\ntrain_rows \u003d train.count()\ntest_rows \u003d test.count()\nprint(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# \u003d\u003d\u003d Define model \u003d\u003d\u003d\nrf \u003d RandomForestClassifier(featuresCol\u003d\"features\", labelCol\u003d\"has_accidents\", seed\u003d42, numTrees\u003d100, maxDepth\u003d5)\n\n# \u003d\u003d\u003d Build pipeline \u003d\u003d\u003d\npipeline_rf \u003d Pipeline(stages\u003dindexers + encoders + [num_assembler, scaler, final_assembler, rf])"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Create Evaluate Objects For Model Measurement Performance\nevaluator_auc \u003d BinaryClassificationEvaluator(labelCol\u003d\"has_accidents\", rawPredictionCol\u003d\"rawPrediction\", metricName\u003d\"areaUnderROC\")\nevaluator_accuracy \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"accuracy\"\n)\nevaluator_precision \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"weightedPrecision\"\n)\nevaluator_recall \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"weightedRecall\"\n)\nevaluator_f1 \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"f1\"\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Set up hyperparameter grid (optional but useful)\nparamGrid \u003d (ParamGridBuilder()\n    .addGrid(rf.numTrees, [50, 100])\n    .addGrid(rf.maxDepth, [5, 10])\n    .build())"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Define \u0026 Create CrossValidator\ncv \u003d CrossValidator(estimator\u003dpipeline_rf,\n                       estimatorParamMaps\u003dparamGrid,\n                       evaluator\u003devaluator_auc, numFolds\u003d3)\n\n# Train model using Cross Validation\ncv_start \u003d time.time()\ncvModel \u003d cv.fit(train)\ncv_end \u003d time.time()\n\ncv_time \u003d cv_end - cv_start\n\n# Make predictions\ncv_predictions \u003d cvModel.transform(test)\n\n# Evaluate\ncv_auc \u003d evaluator_auc.evaluate(cv_predictions)\ncv_accuracy \u003d evaluator_accuracy.evaluate(cv_predictions)\ncv_precision \u003d evaluator_precision.evaluate(cv_predictions)\ncv_recall \u003d evaluator_recall.evaluate(cv_predictions)\ncv_f1 \u003d evaluator_f1.evaluate(cv_predictions)\n\nprint(f\"Cross-Validated AUC (RandomForest Regression): {cv_auc:.4f}\")\nprint(f\"Cross-Validated Accuracy (RandomForest Regression): {cv_accuracy:.4f}\")\nprint(f\"Cross-Validated Precision (RandomForest Regression): {cv_precision:.4f}\")\nprint(f\"Cross-Validated Recall (RandomForest Regression): {cv_recall:.4f}\")\nprint(f\"Cross-Validated F1 (RandomForest Regression): {cv_f1:.4f}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 5. Define \u0026 Create TrainValidationSplit\ntvs \u003d TrainValidationSplit(estimator\u003dpipeline_rf,\n                           estimatorParamMaps\u003dparamGrid,\n                           evaluator\u003devaluator_auc,\n                           trainRatio\u003d0.8)\n\n# 6. Train model using Train Validation\ntvs_start \u003d time.time()\ntvsModel \u003d tvs.fit(train)\ntvs_end \u003d time.time()\n\ntvs_time \u003d tvs_end - tvs_start\n\n# 7. Make predictions\ntvs_predictions \u003d tvsModel.transform(test)\n\n# 8. Evaluate\ntvs_auc \u003d evaluator_auc.evaluate(tvs_predictions)\ntvs_accuracy \u003d evaluator_accuracy.evaluate(tvs_predictions)\ntvs_precision \u003d evaluator_precision.evaluate(tvs_predictions)\ntvs_recall \u003d evaluator_recall.evaluate(tvs_predictions)\ntvs_f1 \u003d evaluator_f1.evaluate(tvs_predictions)\n\nprint(f\"Train Validation Split AUC (RandomForest Regression): {tvs_auc:.4f}\")\nprint(f\"Train Validation Split Accuracy (RandomForest Regression): {tvs_accuracy:.4f}\")\nprint(f\"Train Validation Split Precision (RandomForest Regression): {tvs_precision:.4f}\")\nprint(f\"Train Validation Split Recall (RandomForest Regression): {tvs_recall:.4f}\")\nprint(f\"Train Validation Split F1 (RandomForest Regression): {tvs_f1:.4f}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# --- Summary of All Results ---\nprint(f\"\\n--- Summary of All Results ---\")\nprint(f\"Training Rows: {train_rows:,}\\nTesting Rows: {test_rows:,}\")\n\nprint(f\"\\n--- Summary of Cross-Validation Results ---\")\nprint(f\"CV AUC: {cv_auc:.4f}\")\nprint(f\"CV Accuracy: {cv_accuracy:.4f}\")\nprint(f\"CV Precision: {cv_precision:.4f}\")\nprint(f\"CV Recall: {cv_recall:.4f}\")\nprint(f\"CV F1: {cv_f1:.4f}\")\nprint(f\"CV Time: {cv_time:.2f} seconds\")\n\nprint(f\"\\n--- Summary of Train-Validation-Split Results ---\")\nprint(f\"TVS AUC: {tvs_auc:.4f}\")\nprint(f\"TVS Accuracy: {tvs_accuracy:.4f}\")\nprint(f\"TVS Precision: {tvs_precision:.4f}\")\nprint(f\"TVS Recall: {tvs_recall:.4f}\")\nprint(f\"TVS F1: {tvs_f1:.4f}\")\nprint(f\"TVS Time: {tvs_time:.2f} seconds\")"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Get the trained RandomForest model\nrf_model \u003d cvModel.bestModel.stages[-1]\n\n# Extract feature importances\nimportances \u003d rf_model.featureImportances.toArray()\n\n# Get metadata from the predictions DataFrame\nmetadata \u003d cv_predictions.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]\n\n# Rebuild feature name list from metadata\noriginal_feature_names \u003d []\nfor attr_type in [\"binary\", \"numeric\"]:\n    if attr_type in metadata:\n        original_feature_names +\u003d [attr[\"name\"] for attr in metadata[attr_type]]\n\n# Build mapping for scaled features\nscaled_feature_map \u003d {f\"scaled_features_{i}\": name for i, name in enumerate(numeric_features)}\n\n# Rename all features\nrenamed_features \u003d []\nfor name, score in zip(original_feature_names, importances):\n    # Scaled numeric feature: scaled_features_#\n    if name in scaled_feature_map:\n        readable_name \u003d scaled_feature_map[name]\n    # One-hot encoded fields (categorical)\n    elif name.startswith(\"dealer_zip_vec_\"):\n        readable_name \u003d f\"dealer_zip \u003d {name.replace(\u0027dealer_zip_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"make_name_vec_\"):\n        readable_name \u003d f\"make_name \u003d {name.replace(\u0027make_name_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"fuel_type_vec_\"):\n        readable_name \u003d f\"fuel_type \u003d {name.replace(\u0027fuel_type_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"transmission_vec_\"):\n        readable_name \u003d f\"transmission \u003d {name.replace(\u0027transmission_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"body_type_vec_\"):\n        readable_name \u003d f\"body_type \u003d {name.replace(\u0027body_type_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"model_name_vec_\"):\n        readable_name \u003d f\"model_name \u003d {name.replace(\u0027model_name_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"exterior_color_vec_\"):\n        readable_name \u003d f\"exterior_color \u003d {name.replace(\u0027exterior_color_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"interior_color_vec_\"):\n        readable_name \u003d f\"interior_color \u003d {name.replace(\u0027interior_color_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"franchise_make_vec_\"):\n        readable_name \u003d f\"franchise_make \u003d {name.replace(\u0027franchise_make_vec_\u0027, \u0027\u0027)}\"\n    elif name.startswith(\"wheel_system_vec_\"):\n        readable_name \u003d f\"wheel_system \u003d {name.replace(\u0027wheel_system_vec_\u0027, \u0027\u0027)}\"\n    else:\n        readable_name \u003d name\n\n    renamed_features.append((readable_name, score))\n\n# Sort and limit to top 10\ntop_features \u003d sorted(renamed_features, key\u003dlambda x: x[1], reverse\u003dTrue)[:20]\n\n# Print result\nprint(\"Top 10 Feature Importances (Readable Names):\")\nfor name, score in top_features:\n    print(f\"{name}: {score:.4f}\")\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}