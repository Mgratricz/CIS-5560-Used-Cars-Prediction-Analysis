{
  "metadata": {
    "name": "term-project-logistic-regression.ipynb",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## CIS5560: Final Term Project -\u003e Logistic Regression\n\n### Andrew Pang (apang5@calstatela.edu)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.sql.functions import regexp_extract, col, when\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\nimport time\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Configure Settings for spark-submit"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# True when to create Python soure code to run with spark-submit \nIS_SPARK_SUBMIT_CLI \u003d False\n\nif IS_SPARK_SUBMIT_CLI:\n    sc \u003d SparkContext.getOrCreate()\n    spark \u003d SparkSession(sc)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Import and Parse the Dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# File location and type\nfile_location \u003d \"/user/apang5/used_cars_sample_data--01percent.csv\"\n#file_location \u003d \"/user/apang5/used_cars_data.csv\"\nfile_type \u003d \"csv\"\n\n# CSV options\ninfer_schema \u003d \"true\"\nfirst_row_is_header \u003d \"true\"\ndelimiter \u003d \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf \u003d spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Load the CSV with safe parsing options\ndf \u003d spark.read.format(file_type) \\\n .option(\"header\", \"true\") \\\n .option(\"inferSchema\", \"true\") \\\n .option(\"sep\", \",\") \\\n .option(\"quote\", \"\\\"\") \\\n .option(\"escape\", \"\\\"\") \\\n .option(\"multiLine\", \"true\") \\\n .option(\"mode\", \"PERMISSIVE\") \\\n .load(file_location)\n\ndf.printSchema()\ndf.show(5, truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Feature Handling"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# -----------------------------------------\n# 1. Data Preparation (cleaning, as before)\n# -----------------------------------------\ndata \u003d df.select(\"city_fuel_economy\", \"highway_fuel_economy\", \"daysonmarket\",\n                 \"engine_displacement\", \"horsepower\", \"mileage\", \"seller_rating\",\n                 \"year\", \"price\", \"engine_cylinders\", \"torque\", \"power\",\n                 \"front_legroom\", \"wheelbase\", \"width\", \"body_type\", \"has_accidents\", \n                 \u0027fuel_type\u0027, \u0027transmission\u0027, \u0027make_name\u0027, \u0027model_name\u0027, \u0027exterior_color\u0027,\n                 \u0027interior_color\u0027, \u0027dealer_zip\u0027, \u0027franchise_make\u0027, \u0027wheel_system\u0027)\n\n# Cast numeric columns properly\ndata \u003d data.withColumn(\"city_fuel_economy\", col(\"city_fuel_economy\").cast(\"double\")) \\\n           .withColumn(\"highway_fuel_economy\", col(\"highway_fuel_economy\").cast(\"double\")) \\\n           .withColumn(\"daysonmarket\", col(\"daysonmarket\").cast(\"int\")) \\\n           .withColumn(\"engine_displacement\", col(\"engine_displacement\").cast(\"double\")) \\\n           .withColumn(\"horsepower\", col(\"horsepower\").cast(\"double\")) \\\n           .withColumn(\"mileage\", col(\"mileage\").cast(\"double\")) \\\n           .withColumn(\"seller_rating\", col(\"seller_rating\").cast(\"double\")) \\\n           .withColumn(\"year\", col(\"year\").cast(\"int\")) \\\n           .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n           .withColumn(\"engine_cylinders\", regexp_extract(\"engine_cylinders\", \"(\\\\d+)\", 1).cast(\"double\")) \\\n           .withColumn(\"torque\", regexp_extract(\"torque\", \"(\\\\d+)\", 1).cast(\"double\")) \\\n           .withColumn(\"power\", regexp_extract(\"power\", \"(\\\\d+)\", 1).cast(\"double\"))\n\nfor col_name in [\"front_legroom\", \"wheelbase\", \"width\"]:\n    data \u003d data.withColumn(col_name, regexp_extract(col_name, \"(\\\\d+\\\\.\\\\d+)\", 1).cast(\"double\"))\n\ndata \u003d data.withColumn(\"has_accidents\", col(\"has_accidents\").cast(\"int\"))\ndata \u003d data.dropna()\ndata.groupBy(\"has_accidents\").count().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# -----------------------------------------\n# Feature Preparation\n# -----------------------------------------\nfeature_cols \u003d [\n    \u0027city_fuel_economy\u0027, \u0027highway_fuel_economy\u0027, \u0027engine_displacement\u0027,\n    \u0027horsepower\u0027, \u0027seller_rating\u0027, \u0027year\u0027, \u0027mileage\u0027,\n    \u0027engine_cylinders\u0027, \u0027torque\u0027, \u0027power\u0027, \u0027front_legroom\u0027, \u0027wheelbase\u0027, \u0027width\u0027\n]\n\nnumeric_features \u003d [\n    \u0027city_fuel_economy\u0027, \u0027highway_fuel_economy\u0027, \u0027engine_displacement\u0027,\n    \u0027horsepower\u0027, \u0027seller_rating\u0027, \u0027year\u0027, \u0027mileage\u0027,\n    \u0027engine_cylinders\u0027, \u0027torque\u0027, \u0027power\u0027, \u0027front_legroom\u0027, \u0027wheelbase\u0027, \u0027width\u0027\n]\n\ncategorical_features \u003d [\n    \u0027body_type\u0027, \u0027fuel_type\u0027, \u0027transmission\u0027, \u0027make_name\u0027, \u0027model_name\u0027, \u0027exterior_color\u0027,\n    \u0027interior_color\u0027, \u0027dealer_zip\u0027, \u0027franchise_make\u0027, \u0027wheel_system\u0027\n]"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# \u003d\u003d\u003d Index + One-Hot Encode categorical features \u003d\u003d\u003d\nindexers \u003d [StringIndexer(inputCol\u003dcol, outputCol\u003df\"{col}_idx\", handleInvalid\u003d\"keep\") for col in categorical_features]\nencoders \u003d [OneHotEncoder(inputCol\u003df\"{col}_idx\", outputCol\u003df\"{col}_vec\") for col in categorical_features]\nencoded_features \u003d [f\"{col}_vec\" for col in categorical_features]\n\n# \u003d\u003d\u003d Assemble and scale numeric features \u003d\u003d\u003d\nnum_assembler \u003d VectorAssembler(inputCols\u003dnumeric_features, outputCol\u003d\"num_features\")\nscaler \u003d MinMaxScaler(inputCol\u003d\"num_features\", outputCol\u003d\"scaled_features\")\n\n# \u003d\u003d\u003d Final feature assembler \u003d\u003d\u003d\nfinal_assembler \u003d VectorAssembler(inputCols\u003dencoded_features + [\"scaled_features\"], outputCol\u003d\"features\")\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Split the Dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsplits \u003d data.randomSplit([0.7, 0.3])\ntrain \u003d splits[0]\ntest \u003d splits[1]\ntrain_rows \u003d train.count()\ntest_rows \u003d test.count()\nprint(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Build the Model"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# \u003d\u003d\u003d Define model \u003d\u003d\u003d\nlr \u003d LogisticRegression(featuresCol\u003d\"features\", labelCol\u003d\"has_accidents\", maxIter\u003d10,regParam\u003d0.3)\n\n# \u003d\u003d\u003d Build pipeline \u003d\u003d\u003d\npipeline_lr \u003d Pipeline(stages\u003dindexers + encoders + [num_assembler, scaler, final_assembler, lr])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Build the ParamGridBuilder and the CrossValidator"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Set up hyperparameter grid (optional but useful)\nparamGrid \u003d (ParamGridBuilder() \\\n             .addGrid(lr.regParam, [0.01, 0.5]) \\\n             .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n             .addGrid(lr.maxIter, [1, 5]) \\\n             .build())\n\n# Define CrossValidator\nevaluator_auc \u003d BinaryClassificationEvaluator(labelCol\u003d\"has_accidents\", rawPredictionCol\u003d\"rawPrediction\", metricName\u003d\"areaUnderROC\")\nevaluator_accuracy \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"accuracy\"\n)\nevaluator_precision \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"weightedPrecision\"\n)\nevaluator_recall \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"weightedRecall\"\n)\nevaluator_f1 \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"f1\"\n)\n\ncv \u003d CrossValidator(estimator\u003dpipeline_lr,\n                       estimatorParamMaps\u003dparamGrid,\n                       evaluator\u003devaluator_auc, numFolds\u003d3)\n\n# Train model using Cross Validation\ncv_start \u003d time.time()\ncvModel \u003d cv.fit(train)\ncv_end \u003d time.time()\n\ncv_time \u003d cv_end - cv_start\n\n# Make predictions\ncv_predictions \u003d cvModel.transform(test)\n\n# Evaluate\ncv_auc \u003d evaluator_auc.evaluate(cv_predictions)\ncv_accuracy \u003d evaluator_accuracy.evaluate(cv_predictions)\ncv_precision \u003d evaluator_precision.evaluate(cv_predictions)\ncv_recall \u003d evaluator_recall.evaluate(cv_predictions)\ncv_f1 \u003d evaluator_f1.evaluate(cv_predictions)\n\nprint(f\"Cross-Validated AUC (Logistic Regression): {cv_auc:.4f}\")\nprint(f\"Cross-Validated Accuracy (Logistic Regression): {cv_accuracy:.4f}\")\nprint(f\"Cross-Validated Precision (Logistic Regression): {cv_precision:.4f}\")\nprint(f\"Cross-Validated Recall (Logistic Regression): {cv_recall:.4f}\")\nprint(f\"Cross-Validated F1 (Logistic Regression): {cv_f1:.4f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Build the ParamGridBuilder and the TrainValidationSplit"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# paramGrid \u003d (ParamGridBuilder()\n#              .addGrid(lr.regParam, [0.001, 0.01, 0.1, 1.0])\n#              .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n#              .build())\n\n# 5. Define TrainValidationSplit\nevaluator_auc \u003d BinaryClassificationEvaluator(labelCol\u003d\"has_accidents\", rawPredictionCol\u003d\"rawPrediction\", metricName\u003d\"areaUnderROC\")\nevaluator_accuracy \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"accuracy\"\n)\nevaluator_precision \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"weightedPrecision\"\n)\nevaluator_recall \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"weightedRecall\"\n)\nevaluator_f1 \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"has_accidents\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"f1\"\n)\n\n# 5. Create TrainValidationSplit\ntvs \u003d TrainValidationSplit(estimator\u003dpipeline_lr,\n                           estimatorParamMaps\u003dparamGrid,\n                           evaluator\u003devaluator_auc,\n                           trainRatio\u003d0.8)\n\n# 6. Train model using Train Validation\ntvs_start \u003d time.time()\ntvsModel \u003d tvs.fit(train)\ntvs_end \u003d time.time()\n\ntvs_time \u003d tvs_end - tvs_start\n\n# 7. Make predictions\ntvs_predictions \u003d tvsModel.transform(test)\n\n# 8. Evaluate\ntvs_auc \u003d evaluator_auc.evaluate(tvs_predictions)\ntvs_accuracy \u003d evaluator_accuracy.evaluate(tvs_predictions)\ntvs_precision \u003d evaluator_precision.evaluate(tvs_predictions)\ntvs_recall \u003d evaluator_recall.evaluate(tvs_predictions)\ntvs_f1 \u003d evaluator_f1.evaluate(tvs_predictions)\n\nprint(f\"Train Validation Split AUC (Logistic Regression): {tvs_auc:.4f}\")\nprint(f\"Train Validation Split Accuracy (Logistic Regression): {tvs_accuracy:.4f}\")\nprint(f\"Train Validation Split Precision (Logistic Regression): {tvs_precision:.4f}\")\nprint(f\"Train Validation Split Recall (Logistic Regression): {tvs_recall:.4f}\")\nprint(f\"Train Validation Split F1 (Logistic Regression): {tvs_f1:.4f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Summarize the Evaluation Results"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# --- Summary of All Results ---\nprint(f\"\\n--- Summary of All Results ---\")\nprint(f\"Training Rows: {train_rows:,}\\nTesting Rows: {test_rows:,}\")\n\nprint(f\"\\n--- Summary of Cross-Validation Results ---\")\nprint(f\"CV AUC: {cv_auc:.4f}\")\nprint(f\"CV Accuracy: {cv_accuracy:.4f}\")\nprint(f\"CV Precision: {cv_precision:.4f}\")\nprint(f\"CV Recall: {cv_recall:.4f}\")\nprint(f\"CV F1: {cv_f1:.4f}\")\nprint(f\"CV Time: {cv_time:.2f} seconds\")\n\nprint(f\"\\n--- Summary of Train-Validation-Split Results ---\")\nprint(f\"TVS AUC: {tvs_auc:.4f}\")\nprint(f\"TVS Accuracy: {tvs_accuracy:.4f}\")\nprint(f\"TVS Precision: {tvs_precision:.4f}\")\nprint(f\"TVS Recall: {tvs_recall:.4f}\")\nprint(f\"TVS F1: {tvs_f1:.4f}\")\nprint(f\"TVS Time: {tvs_time:.2f} seconds\")"
    }
  ]
}