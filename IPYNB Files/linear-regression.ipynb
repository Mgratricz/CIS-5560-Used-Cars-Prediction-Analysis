{
  "metadata": {
    "name": "term-project-linear-regression.ipynb",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## CIS5560: Final Term Project -\u003e Linear Regression\n\n### Andrew Pang (apang5@calstatela.edu)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.linalg import Vectors, SparseVector\nfrom pyspark.ml.clustering import LDA, BisectingKMeans\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.sql.functions import regexp_extract, col\nfrom pyspark.ml.tuning import CrossValidator, TrainValidationSplit, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml import Pipeline\nimport time\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\nfrom pyspark.ml.regression import LinearRegression\n\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\n\nimport re"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Configure Settings for spark-submit"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# True when to create Python soure code to run with spark-submit \nIS_SPARK_SUBMIT_CLI \u003d False\n\nif IS_SPARK_SUBMIT_CLI:\n    sc \u003d SparkContext.getOrCreate()\n    spark \u003d SparkSession(sc)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Import and Parse the Dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# File location and type\nfile_location \u003d \"/user/apang5/used_cars_sample_data--01percent.csv\"\nfile_type \u003d \"csv\"\n\n\n# Load the CSV with safe parsing options\ndf \u003d spark.read.format(file_type) \\\n .option(\"header\", \"true\") \\\n .option(\"inferSchema\", \"true\") \\\n .option(\"sep\", \",\") \\\n .option(\"quote\", \"\\\"\") \\\n .option(\"escape\", \"\\\"\") \\\n .option(\"multiLine\", \"true\") \\\n .option(\"mode\", \"PERMISSIVE\") \\\n .load(file_location)\n\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Create a view or table\n\n#temp_table_name \u003d \"used_cars_sample_data_csv\"\n\n#df.createOrReplaceTempView(temp_table_name)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Select Columns/Features"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ncategorical_cols \u003d [\"body_type\",\"engine_type\",\"make_name\",\"fuel_type\",\"maximum_seating\",\"model_name\",\"wheel_system\",\"year\"]\nnumeric_cols \u003d [\"city_fuel_economy\",\"highway_fuel_economy\",\"mileage\"]\n\ncolumns \u003d categorical_cols + numeric_cols\n\ndata \u003d df.select(columns + [\"price\"])\n\nprint(data.columns)\n\nprint(f\u0027# of Rows: {data.count()}\\n# of Columns: {len(data.columns)}\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Split the Dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsplits \u003d data.randomSplit([0.7, 0.3])\ntrain \u003d splits[0]\ntest \u003d splits[1]\ntrain_rows \u003d train.count()\ntest_rows \u003d test.count()\nprint(\"Training Rows:\", train_rows, \"\\nTesting Rows:\", test_rows)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Preprocess the Categorical Features"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Select categorical columns\n\n# Handle the nulls in categorical columns by replacing nulls with \u0027unknown\u0027\nfor col_name in categorical_cols:\n    data \u003d data.fillna({col_name: \u0027unknown\u0027})\n    \n# Step 1: Convert string categories to numeric indices\nindexers \u003d [StringIndexer(inputCol \u003d col,\n                          outputCol \u003d col + \"_index\",\n                          handleInvalid \u003d \"skip\") for col in categorical_cols]\n                          \n\n# Step 2: One-hot encode the indices\nencoder \u003d OneHotEncoder(\n    inputCols\u003d[col + \"_index\" for col in categorical_cols],\n    outputCols\u003d[col + \"_vec\" for col in categorical_cols]\n)\n\n# Step 3: Assemble one-hot encoded vectors into a single feature vector\nencoded_cols \u003d [col + \"_vec\" for col in categorical_cols]\n\n# Assemble categorical vector\ncategorical_VectorAssembler \u003d VectorAssembler(\n    inputCols\u003d[f\"{col}_vec\" for col in categorical_cols],\n    outputCol\u003d\"catFeatures\"\n)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Preprocess the Numerical Features"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Cast numeric columns\nfor col_name in numeric_cols:\n    data \u003d data.withColumn(col_name, col(col_name).cast(\u0027double\u0027))\n\n# Handle the nulls in numeric columns with the mean\nfor col_name in numeric_cols:\n    mean_value \u003d data.selectExpr(f\"avg({col_name}) as mean\").first()[\"mean\"]\n    data \u003d data.na.fill({col_name: mean_value})\n\n# Assemble numeric features\nnumeric_assembler \u003d VectorAssembler(inputCols\u003dnumeric_cols, outputCol\u003d\"numeric_features\", handleInvalid \u003d \"skip\")\n\n# Scale numeric features\nscaler \u003d MinMaxScaler(inputCol\u003d\"numeric_features\", outputCol\u003d\"scaled_features\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Combine Categorical \u0026 Numeric Features"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Final assembler: combine scaled numeric + categorical\nfeatVect \u003d VectorAssembler(inputCols\u003d[\"catFeatures\", \"scaled_features\"], outputCol\u003d\"features\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Define the Model"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Use Linear Regression instead of Decision Tree Classifier\nlr \u003d LinearRegression(featuresCol\u003d\"features\", labelCol\u003d\"price\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Define the Pipeline"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline \u003d Pipeline(stages\u003dindexers + [encoder, categorical_VectorAssembler, numeric_assembler, scaler, featVect, lr])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Build the ParamGridBuilder"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nparamGrid \u003d ParamGridBuilder() \\\n  .addGrid(lr.regParam, [0.0, 0.01, 0.1]) \\\n  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n  .addGrid(lr.maxIter, [10, 50]) \\\n  .build()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Train Validation Split"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntvs \u003d TrainValidationSplit(estimator\u003dpipeline, estimatorParamMaps\u003dparamGrid, evaluator\u003dRegressionEvaluator(labelCol\u003d\"price\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\"))\n                    \nstart \u003d time.time()\ntvsModel \u003d tvs.fit(train)\nend \u003d time.time()\n\ntvs_time \u003d end - start\n\ntvs_predictions \u003d tvsModel.transform(test)\n\nprint(f\"tvsModel fit time: {tvs_time:.2f} seconds\")\n\n# EVAL\n\nevaluator_rmse \u003d RegressionEvaluator(labelCol\u003d\"price\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\nevaluator_r2 \u003d RegressionEvaluator(labelCol\u003d\"price\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\")\n\ntvs_rmse \u003d evaluator_rmse.evaluate(tvs_predictions)\ntvs_r2 \u003d evaluator_r2.evaluate(tvs_predictions)\n\nprint(\"TVS RMSE \u003d\", tvs_rmse)\nprint(\"TVS R² \u003d\", tvs_r2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Cross Validator"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ncv \u003d CrossValidator(estimator\u003dpipeline, estimatorParamMaps\u003dparamGrid, evaluator\u003dRegressionEvaluator(labelCol\u003d\"price\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\"), numFolds\u003d3)\n\n\n\nstart \u003d time.time()\ncvModel \u003d cv.fit(train)\nend \u003d time.time()\n\ncv_time \u003d end - start\n\ncv_predictions \u003d cvModel.transform(test)\n\nprint(f\"cvModel fit time: {cv_time:.2f} seconds\")\n\n# EVAL\n\nevaluator_rmse \u003d RegressionEvaluator(labelCol\u003d\"price\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\nevaluator_r2 \u003d RegressionEvaluator(labelCol\u003d\"price\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\")\n\ncv_rmse \u003d evaluator_rmse.evaluate(cv_predictions)\ncv_r2 \u003d evaluator_r2.evaluate(cv_predictions)\n\nprint(\"CV RMSE \u003d\", cv_rmse)\nprint(\"CV R² \u003d\", cv_r2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Extract the Coefficients/Hyper-Parameters from the Best Model"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Note: LinearRegression uses coefficients, not featureImportances\n\n# 1. Define feature names in the same order they were fed into VectorAssembler\nall_feature_names \u003d numeric_cols + [col + \"_index\" for col in categorical_cols]\n\nfinalStageModel \u003d cvModel.bestModel.stages[-1]\n\n# 2. Get coefficients\ncoefficients \u003d finalStageModel.coefficients.toArray()\n\n# 3. Create DataFrame\nimport pandas as pd\nfeatureImp \u003d pd.DataFrame(\n    zip(all_feature_names, coefficients),\n    columns\u003d[\"feature\", \"coefficient\"]\n)\n\n# 4. Sort by importance\nfeatureImp \u003d featureImp.reindex(featureImp.coefficient.abs().sort_values(ascending\u003dFalse).index)\n\n# 5. Print\nprint(\"Best regParam:\", finalStageModel.getRegParam())\nprint(\"Best elasticNetParam:\", finalStageModel.getElasticNetParam())\nprint(featureImp)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Summarize the Evaluation Results"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# --- Summary of All Results ---\nprint(f\"\\n--- Summary of All Results ---\")\nprint(f\"Training Rows: {train_rows:,}\\nTesting Rows: {test_rows:,}\")\n\nprint(f\"\\n--- Summary of Cross-Validation Results ---\")\nprint(f\"Cross-Validated RMSE: {cv_rmse:.4f}\")\nprint(f\"Cross-Validated R2:   {cv_r2:.4f}\")\nprint(f\"CV Time: {cv_time:.2f} seconds\")\n\nprint(f\"\\n--- Summary of Train-Validation-Split Results ---\")\nprint(f\"TrainValidationSplit RMSE: {tvs_rmse:.4f}\")\nprint(f\"TrainValidationSplit R2:   {tvs_r2:.4f}\")\nprint(f\"TVS Time: {tvs_time:.2f} seconds\")\n"
    }
  ]
}